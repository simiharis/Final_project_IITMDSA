{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGLBdN44IgtO",
        "outputId": "eadce871-fe97-4d50-ccf0-122183faa46b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer architecture to do NLP task\n",
        "!pip install transformers torch\n",
        "!pip install diffusers\n",
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw3gm-1EI5Jc",
        "outputId": "5c143120-fe59-4fe1-e331-1ab2818ed060"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.26.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (11.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Hugging_face.py\n",
        "# Importing necessary packages\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import streamlit as st\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "st.title(':black[\"Hugging Face Model Hub: Real-Time NLP in Action\"]')\n",
        "st.write('''Explore NLP tasks like sentiment analysis, text generation, and more with Hugging Face models in real-time.''')\n",
        "# Task selection\n",
        "task = st.sidebar.selectbox('Choose a task', ['Text Summarization', 'Next Word Prediction',\n",
        "                                              'Story Prediction', 'Chatbot',\n",
        "                                              'Sentiment Analysis', 'Question Answering',\n",
        "                                              'Image Generation'])\n",
        "\n",
        "if task=='Text Summarization':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      model_name = \"facebook/bart-large-cnn\"\n",
        "      tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "      model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      def summarize(text, model, tokenizer, max_length=200, min_length=30, do_sample=False):\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "      summary = summarize(user_input, model, tokenizer)\n",
        "      st.write(\"\\nSummary:\\n\", summary)\n",
        "\n",
        "elif task=='Next Word Prediction':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    def predict_next_word(prompt, model, tokenizer, top_k=5):\n",
        "      # Tokenize input text\n",
        "      inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "      # Generate predictions\n",
        "      with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "      # Get logits of the last token in the sequence\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "      # Filter the top k tokens by their probability\n",
        "      top_k_tokens = torch.topk(next_token_logits, top_k).indices[0].tolist()\n",
        "\n",
        "      # Decode the top k tokens to their corresponding words\n",
        "      predicted_tokens = [tokenizer.decode([token]) for token in top_k_tokens]\n",
        "\n",
        "      return predicted_tokens\n",
        "    predicted_words = predict_next_word(user_input, model, tokenizer)\n",
        "    st.write(f\"Next word predictions: {predicted_words}\")\n",
        "\n",
        "elif task=='Story Prediction':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      story_predictor = pipeline('text-generation', model='gpt2')\n",
        "      prediction = story_predictor(user_input,max_length=1000,truncation=True,clean_up_tokenization_spaces=True)\n",
        "      st.write(prediction[0]['generated_text'])\n",
        "\n",
        "elif task=='Chatbot':\n",
        "  # Load pre-trained model and tokenizer with padding set to left\n",
        "  model_name = \"microsoft/DialoGPT-medium\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  def chat_with_model(prompt, chat_history_ids, model, tokenizer):\n",
        "    # Encode the new user input, add the eos_token and return a tensor in PyTorch\n",
        "    new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
        "\n",
        "    # Generate a response\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the response and print it\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response, chat_history_ids\n",
        "\n",
        "  # Initialize chat history\n",
        "  chat_history_ids = None\n",
        "\n",
        "  st.write(\"Start chatting with the bot (type 'quit' to stop)\")\n",
        "  keyz = 's1'\n",
        "  user_input = st.text_input(\"You: \",key = keyz)\n",
        "  while user_input.lower() != 'quit':\n",
        "    response, chat_history_ids = chat_with_model(user_input, chat_history_ids, model, tokenizer)\n",
        "    if (response != '')and(user_input != ''):\n",
        "      st.write(f\"Bot: {response}\")\n",
        "      keyz += '1'\n",
        "      user_input = st.text_input(\"You: \",key = keyz)\n",
        "    #response = ''\n",
        "\n",
        "elif task=='Sentiment Analysis':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      # Load pre-trained sentiment-analysis pipeline\n",
        "      sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "      def analyze_sentiment(texts):\n",
        "        results = sentiment_analysis(texts)\n",
        "        for text, result in zip(texts, results):\n",
        "          st.write(f\"Text: {text}\")\n",
        "          st.write(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\")\n",
        "\n",
        "      # Example texts for sentiment analysis\n",
        "      sent_texts = re.split(r'([.!?])', user_input)\n",
        "      resultz = [sent_texts[i] + sent_texts[i+1] for i in range(0, len(sent_texts)-1, 2)]\n",
        "      # Perform sentiment analysis\n",
        "      for text in resultz:\n",
        "        analyze_sentiment(text)\n",
        "\n",
        "elif task=='Question Answering':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  question = st.text_input('Enter a Question ')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      # Load the pre-trained model and tokenizer\n",
        "      model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "      def answer_question(question, context):\n",
        "        # Tokenize input question and context\n",
        "        inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Get input ids and attention mask\n",
        "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "        # Get the model's output\n",
        "        outputs = model(**inputs)\n",
        "        answer_start_scores = outputs.start_logits\n",
        "        answer_end_scores = outputs.end_logits\n",
        "\n",
        "        # Get the most likely beginning and end of answer with the argmax of the score\n",
        "        answer_start = torch.argmax(answer_start_scores)\n",
        "        answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "        # Convert tokens to the answer\n",
        "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "        return answer\n",
        "\n",
        "      # Define a context and a question\n",
        "      context = user_input\n",
        "\n",
        "      # Get the answer\n",
        "      answer = answer_question(question, context)\n",
        "      st.write(f\"Answer: {answer}\")\n",
        "\n",
        "elif task=='Image Generation':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      # Load the Stable Diffusion model\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(\"cuda\")\n",
        "\n",
        "      # Function to generate images\n",
        "      def generate_image(prompt):\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "      # Example text prompt\n",
        "      prompt = user_input\n",
        "\n",
        "      # Generate image\n",
        "      image = generate_image(prompt)\n",
        "\n",
        "      # Display the generated image\n",
        "      st.image(image)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Or4jW-uJrT_",
        "outputId": "2f4d499d-e0f0-463f-b0ce-f730ab89eb55"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Hugging_face.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dxShys0J8RS",
        "outputId": "c7dfab56-e64b-430e-d1a6-84c919433c71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "up to date, audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlOXq7xVKIzw",
        "outputId": "57f3dac0-bacd-43e7-cbd7-8e8a8e0569f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.125.216.181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/Hugging_face.py &>/content/logs.txt &\n",
        ""
      ],
      "metadata": {
        "id": "0o1OSgF7KTwM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uje7Ts53Kgyq",
        "outputId": "94d3fcaf-52b3-4112-f856-c69e7163348e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://ripe-brooms-wait.loca.lt\n"
          ]
        }
      ]
    }
  ]
}